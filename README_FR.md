# Setting up an MLOps project step by step üöÄ

Bienvenue dans le guide de configuration ! Ici, nous allons d√©crire les √©tapes n√©cessaires pour configurer et mettre en ≈ìuvre les diff√©rentes premi√®res √©tapes du pipeline MLOps. Suivez-nous et remplissez les d√©tails au fur et √† mesure que vous avancez dans chaque √©tape dans le carnet `workflow_steps.ipynb`.

Vous pouvez commencer par vous familiariser avec l'architecture du projet : 

```bash
+---.dvc
|  
+---.github
|   \---workflows
+---data
|           
+---logs
|       
+---metrics
+---notebooks
|       workflow_steps.ipynb
|       
+---src
|   |   common_utils.py
|   |   config.py
|   |   config.yaml
|   |   config_manager.py
|   |   entity.py
|   |   __init__.py
|   |   
|   +---app
|   |       app.py
|   |       
|   +---data_module_def
|   |   |   data_ingestion.py
|   |   |   data_transformation.py
|   |   |   data_validation.py
|   |   |   schema.yaml
|   |   |   __init__.p
|   |           
|   +---models_module_def
|   |       model_evaluation.py
|   |       model_trainer.py
|   |       params.yaml
|   |       __init__.py
|   |       
|   +---pipeline_steps
|   |       prediction.py
|   |       stage01_data_ingestion.py
|   |       stage02_data_validation.py
|   |       stage03_data_transformation.py
|   |       stage04_model_trainer.py
|   |       stage05_model_evaluation.py
|   |       __init__.py
|           
+---templates
|       index.html
|       login.html
|       register.html
|       results.html
|       
+---users
|   |   users.json
| 
|   .dvcignore
|   .gitignore
|   custom_logger.py
|   dvc.lock
|   dvc.yaml
|   README.md
|   requirements.txt
|   tree
|   __init__.py
|  
```
√Ä travers ce projet, nous allons travailler avec un jeu de donn√©es sur le vin üç∑ L'objectif sera de mettre en place un mod√®le qui permettra de pr√©dire sa qualit√©, le tout en respectant les bonnes pratiques de MLOps en termes de contr√¥le de version, d'utilisation de pipelines et des outils les plus couramment utilis√©s.

Tout d'abord, il faut commencer par forker et cloner le projet. Ensuite, vous devez cr√©er un environnement virtuel dans lequel vous installerez toutes les biblioth√®ques n√©cessaires. Celles-ci se trouvent dans le fichier `requirements.txt` üìö Assurez-vous d'activer votre environnement virtuel avant de l'utiliser üòâ .

Passons maintenant en revue les fichiers facilement disponibles.

## Configuration Files üìò
Regardons rapidement les trois fichiers `yaml` dans notre dossier `src`.

Vous pouvez commencer par regarder le `config.yaml` üìÇ Vous verrez qu'il d√©finit les chemins vers les diff√©rents fichiers qui seront utilis√©s et cr√©√©s dans chacune des √©tapes que nous allons mettre en place.

Ensuite, dans le dossier `data_module_def` nous avons le `schema.yaml` üóÉÔ∏è Si vous y jetez un coup d'oeil vous verrez qu'il d√©finit les types de donn√©es pour chaque colonne dans l'ensemble de donn√©es avec lequel nous allons travailler.

Enfin, dans le dossier `models_module_def` vous pouvez jeter un coup d'oeil √† `params.yaml` üìä Ce fichier d√©finit les hyperparam√®tres du mod√®le que nous allons mettre en place.

‚ö†Ô∏è Le fichier `src/config.py` d√©finit les variables globales contenant les chemins vers ces fichiers yaml pour faciliter leur acc√®s. 

## Common Utilities üõ†Ô∏è 
Dans `src/common_utils.py` nous avons des fonctions r√©utilisables :

* read_yaml(filepath : str) -> dict
* create_directories(paths : List[str])
* save_json(path : str, data : dict)
* load_json

Ces utilitaires simplifieront le chargement des configurations et garantiront la cr√©ation des r√©pertoires n√©cessaires.

Mettons-nous au travail !

## The task
Pour les √©tapes suivantes, vous pouvez utiliser le notebook `workflow_steps.ipynb` pour vous guider √† travers le code que vous devrez √©crire sur chacun des fichiers correspondants üßë‚Äçüíª La t√¢che consiste en cinq √©tapes qui vous aideront √† mettre en ≈ìuvre un flux de travail modulaire d'un projet MLOps.

## Step 1: Define Configuration Classes üß©
Commencez par √©crire les objets de configuration dans `src/entity.py`. Ces configurations aideront √† g√©rer les r√©glages et les param√®tres requis pour chaque √©tape d'une mani√®re propre et organis√©e. En utilisant la section *Etape 1* du notebook, d√©finissez les `dataclasses` pour les objets de configuration :

* DataIngestionConfig
* DataValidationConfig
* DataTransformationConfig
* ModelTrainerConfig
* ModelEvaluationConfig

## Step 2: Configuration Manager üóÑÔ∏è
Cr√©ez la classe `ConfigurationManager` dans `src/config_manager.py` en utilisant l'*Etape 2* du notebook. Cette classe va :

* Lire les chemins depuis `config.yaml`.
* Lire les hyperparam√®tres depuis `params.yaml`.
* Lire les types de donn√©es depuis `schema.yaml`.
* Cr√©er des objets de configuration pour chacune des √©tapes √† l'aide des objets d√©finis √† l'√©tape pr√©c√©dente : DataIngestionConfig, DataValidationConfig, ModelTrainerConfig et ModelEvaluationConfig.
* Cr√©er les dossiers n√©cessaires.

‚ö†Ô∏è Faites attention au `mlflow_uri` sur le `get_model_evaluation_config`, assurez-vous de l'adapter avec vos propres credentials dagshub. 

## Step 3: Data module definition and model module definition.
En utilisant la section *Etape 3* du cahier, dans les fichiers correspondants du dossier `src/data_module_def`, cr√©ez :

1. Module d'ingestion de donn√©es üì•

Cette classe va :
* T√©l√©charger l'ensemble de donn√©es dans le dossier appropri√©.
* D√©zipper le jeu de donn√©es dans le dossier appropri√©.

2. Module de validation des donn√©es ‚úÖ

Cette classe va :
* Valider les colonnes par rapport au sch√©ma. Facultatif : vous pouvez √©galement v√©rifier le type d'information.
* Produire un fichier texte indiquant si les donn√©es sont valides.

3. Module de transformation des donn√©es üîÑ

Cette classe va :
* Diviser les donn√©es en ensembles de formation et de test.
* Enregistrer les fichiers csv correspondants dans le dossier appropri√©.

De m√™me, dans les fichiers correspondants du dossier `src/models_module_def`, cr√©er :

1. Module d'entra√Ænement au mod√®le üèãÔ∏è‚Äç‚ôÇÔ∏è

Cette classe va :
* Entra√Æner le mod√®le en utilisant les hyperparam√®tres sp√©cifi√©s dans `params.yaml`.
* Sauvegarder le mod√®le form√© dans le dossier appropri√©.

2. Module d'√©valuation du mod√®le üìù

Cette classe va
* √âvaluer le mod√®le et les m√©triques d'enregistrement en utilisant MLFlow

## Step 4: Pipeline Steps üöÄ
En utilisant le *Step 4* du notebook, dans `src/pipeline_steps` cr√©ez des scripts pour chaque √©tape du pipeline afin d'instancier et d'ex√©cuter les processus :

* stage01_data_ingestion.py
* stage02_data_validation.py
* stage03_data_transformation.py
* stage04_model_trainer.py
* stage05_model_evaluation.py

Pour chaque script, vous devez compl√©ter les classes avec deux m√©thodes : une `__init__` qui ne fait rien, et une `main` o√π vous devez impl√©menter le code dans chaque section de l'*√âtape 4* du notebook.

## Step 5: Use DVC to connect the different stages of your pipeline ü¶â
Commencez par configurer DagsHub en tant que stockage distant via DVC.

```bash
dvc remote add origin s3://dvc
dvc remote modify origin endpointurl https://dagshub.com/your_username/your_repo.s3 
dvc remote default origin
```

Utilisez dvc pour connecter les diff√©rentes √©tapes de votre pipeline.

Par exemple, la commande pour ajouter la premi√®re √©tape du pipeline est la suivante : 

``bash
dvc stage add -n data_ingestion -d wine_quality/src/pipeline_steps/stage01_data_ingestion.py -d wine_quality/src/config.yaml -o wine_quality/data/raw/winequality-red.csv python wine_quality/src/pipeline_steps/stage01_data_ingestion.py
```
Ajoutez les √©tapes suivantes pour la transformation des donn√©es, la validation des donn√©es, l'entra√Ænement du mod√®le et l'√©valuation du mod√®le.
