# Setting up an MLOps project step by step ðŸš€

Bienvenue dans le guide de configuration ! Ici, nous allons dÃ©crire les Ã©tapes nÃ©cessaires pour configurer et mettre en Å“uvre les diffÃ©rentes premiÃ¨res Ã©tapes du pipeline MLOps. Suivez-nous et remplissez les dÃ©tails au fur et Ã  mesure que vous avancez dans chaque Ã©tape dans le carnet `workflow_steps.ipynb`.

Vous pouvez commencer par vous familiariser avec l'architecture du projet : 

```bash
+---.dvc
|  
+---.github
|   \---workflows
+---data
|           
+---logs
|       
+---metrics
+---notebooks
|       workflow_steps.ipynb
|       
+---src
|   |   common_utils.py
|   |   config.py
|   |   config.yaml
|   |   config_manager.py
|   |   entity.py
|   |   __init__.py
|   |   
|   +---app
|   |       app.py
|   |       
|   +---data_module_def
|   |   |   data_ingestion.py
|   |   |   data_transformation.py
|   |   |   data_validation.py
|   |   |   schema.yaml
|   |   |   __init__.p
|   |           
|   +---models_module_def
|   |       model_evaluation.py
|   |       model_trainer.py
|   |       params.yaml
|   |       __init__.py
|   |       
|   +---pipeline_steps
|   |       prediction.py
|   |       stage01_data_ingestion.py
|   |       stage02_data_validation.py
|   |       stage03_data_transformation.py
|   |       stage04_model_trainer.py
|   |       stage05_model_evaluation.py
|   |       __init__.py
|           
+---templates
|       index.html
|       login.html
|       register.html
|       results.html
|       
+---users
|   |   users.json
| 
|   .dvcignore
|   .gitignore
|   custom_logger.py
|   dvc.lock
|   dvc.yaml
|   README.md
|   requirements.txt
|   tree
|   __init__.py
|  
```
Ã€ travers ce projet, nous allons travailler avec un jeu de donnÃ©es sur le vin ðŸ· L'objectif sera de mettre en place un modÃ¨le qui permettra de prÃ©dire sa qualitÃ©, le tout en respectant les bonnes pratiques de MLOps en termes de contrÃ´le de version, d'utilisation de pipelines et des outils les plus couramment utilisÃ©s.

Tout d'abord, il faut commencer par forker et cloner le projet. Ensuite, vous devez crÃ©er un environnement virtuel dans lequel vous installerez toutes les bibliothÃ¨ques nÃ©cessaires. Celles-ci se trouvent dans le fichier `requirements.txt` ðŸ“š Assurez-vous d'activer votre environnement virtuel avant de l'utiliser ðŸ˜‰ .

Passons maintenant en revue les fichiers facilement disponibles.

## Configuration Files ðŸ“˜
Regardons rapidement les trois fichiers `yaml` dans notre dossier `src`.

Vous pouvez commencer par regarder le `config.yaml` ðŸ“‚ Vous verrez qu'il dÃ©finit les chemins vers les diffÃ©rents fichiers qui seront utilisÃ©s et crÃ©Ã©s dans chacune des Ã©tapes que nous allons mettre en place.

Ensuite, dans le dossier `data_module_def` nous avons le `schema.yaml` ðŸ—ƒï¸ Si vous y jetez un coup d'oeil vous verrez qu'il dÃ©finit les types de donnÃ©es pour chaque colonne dans l'ensemble de donnÃ©es avec lequel nous allons travailler.

Enfin, dans le dossier `models_module_def` vous pouvez jeter un coup d'oeil Ã  `params.yaml` ðŸ“Š Ce fichier dÃ©finit les hyperparamÃ¨tres du modÃ¨le que nous allons mettre en place.

âš ï¸ Le fichier `src/config.py` dÃ©finit les variables globales contenant les chemins vers ces fichiers yaml pour faciliter leur accÃ¨s. 

## Common Utilities ðŸ› ï¸ 
Dans `src/common_utils.py` nous avons des fonctions rÃ©utilisables :

* read_yaml(filepath : str) -> dict
* create_directories(paths : List[str])
* save_json(path : str, data : dict)
* load_json

Ces utilitaires simplifieront le chargement des configurations et garantiront la crÃ©ation des rÃ©pertoires nÃ©cessaires.

Mettons-nous au travail !

## The task
Pour les Ã©tapes suivantes, vous pouvez utiliser le notebook `workflow_steps.ipynb` pour vous guider Ã  travers le code que vous devrez Ã©crire sur chacun des fichiers correspondants ðŸ§‘â€ðŸ’» La tÃ¢che consiste en cinq Ã©tapes qui vous aideront Ã  mettre en Å“uvre un flux de travail modulaire d'un projet MLOps.

## Step 1: Define Configuration Classes ðŸ§©
Commencez par Ã©crire les objets de configuration dans `src/entity.py`. Ces configurations aideront Ã  gÃ©rer les rÃ©glages et les paramÃ¨tres requis pour chaque Ã©tape d'une maniÃ¨re propre et organisÃ©e. En utilisant la section *Etape 1* du notebook, dÃ©finissez les `dataclasses` pour les objets de configuration :

* DataIngestionConfig
* DataValidationConfig
* DataTransformationConfig
* ModelTrainerConfig
* ModelEvaluationConfig

## Step 2: Configuration Manager ðŸ—„ï¸
CrÃ©ez la classe `ConfigurationManager` dans `src/config_manager.py` en utilisant l'*Etape 2* du notebook. Cette classe va :

* Lire les chemins depuis `config.yaml`.
* Lire les hyperparamÃ¨tres depuis `params.yaml`.
* Lire les types de donnÃ©es depuis `schema.yaml`.
* CrÃ©er des objets de configuration pour chacune des Ã©tapes Ã  l'aide des objets dÃ©finis Ã  l'Ã©tape prÃ©cÃ©dente : DataIngestionConfig, DataValidationConfig, ModelTrainerConfig et ModelEvaluationConfig.
* CrÃ©er les dossiers nÃ©cessaires.

âš ï¸ Faites attention au `mlflow_uri` sur le `get_model_evaluation_config`, assurez-vous de l'adapter avec vos propres credentials dagshub. 

## Step 3: Data module definition and model module definition.
En utilisant la section *Etape 3* du cahier, dans les fichiers correspondants du dossier `src/data_module_def`, crÃ©ez :

1. Module d'ingestion de donnÃ©es ðŸ“¥

Cette classe va :
* TÃ©lÃ©charger l'ensemble de donnÃ©es dans le dossier appropriÃ©.
* DÃ©zipper le jeu de donnÃ©es dans le dossier appropriÃ©.

2. Module de validation des donnÃ©es âœ…

Cette classe va :
* Valider les colonnes par rapport au schÃ©ma. Facultatif : vous pouvez Ã©galement vÃ©rifier le type d'information.
* Produire un fichier texte indiquant si les donnÃ©es sont valides.

3. Module de transformation des donnÃ©es ðŸ”„

Cette classe va :
* Diviser les donnÃ©es en ensembles de formation et de test.
* Enregistrer les fichiers csv correspondants dans le dossier appropriÃ©.

De mÃªme, dans les fichiers correspondants du dossier `src/models_module_def`, crÃ©er :

1. Module d'entraÃ®nement au modÃ¨le ðŸ‹ï¸â€â™‚ï¸

Cette classe va :
* EntraÃ®ner le modÃ¨le en utilisant les hyperparamÃ¨tres spÃ©cifiÃ©s dans `params.yaml`.
* Sauvegarder le modÃ¨le formÃ© dans le dossier appropriÃ©.

2. Module d'Ã©valuation du modÃ¨le ðŸ“

Cette classe va
* Ã‰valuer le modÃ¨le et les mÃ©triques d'enregistrement en utilisant MLFlow

## Step 4: Pipeline Steps ðŸš€
En utilisant le *Step 4* du notebook, dans `src/pipeline_steps` crÃ©ez des scripts pour chaque Ã©tape du pipeline afin d'instancier et d'exÃ©cuter les processus :

* stage01_data_ingestion.py
* stage02_data_validation.py
* stage03_data_transformation.py
* stage04_model_trainer.py
* stage05_model_evaluation.py

Pour chaque script, vous devez complÃ©ter les classes avec deux mÃ©thodes : une `__init__` qui ne fait rien, et une `main` oÃ¹ vous devez implÃ©menter le code dans chaque section de l'*Ã‰tape 4* du notebook.

## Step 5: Use DVC to connect the different stages of your pipeline ðŸ¦‰
Commencez par configurer DagsHub en tant que stockage distant via DVC.

Pour initialiser le repo DVC :
``bash
dvc init
``
Initialiser le repo sous S3 :
```bash
dvc remote add origin s3://dvc
dvc remote modify origin endpointurl https://dagshub.com/stephaneMartinez/MLOps_Wines_Model.s3
dvc remote default origin
```

Initilialiser les credentials (dans dagshub "remote" "Data" > "DVC" > "Setup Credentials")
```bash
dvc remote modify origin --local access_key_id ddcdf3d811ab7bb6d3437387618f9956f72a9254
dvc remote modify origin --local secret_access_key ddcdf3d811ab7bb6d3437387618f9956f72a9254
```

Utilisez dvc pour connecter les diffÃ©rentes Ã©tapes de votre pipeline.

Par exemple, la commande pour ajouter la premiÃ¨re Ã©tape du pipeline est la suivante : 


Pour initiliaser le pipeline :
```bash
dvc stage add -n data_ingestion \
              -d src/config.yaml \
              -d src/pipeline_steps/stage01_data_ingestion.py \
              -o data/raw/winequality-red.csv \
              python src/pipeline_steps/stage01_data_ingestion.py
```

```bash
dvc stage add -n data_validation \
              -d src/config.yaml \
              -d data/raw/winequality-red.csv \
              -d  src/pipeline_steps/stage02_data_validation.py \
              -o data/status.txt \
              python src/pipeline_steps/stage02_data_validation.py
```

```bash
dvc stage add -n data_transformation \
              -d src/config.yaml \
              -d data/raw/winequality-red.csv \
              -d data/status.txt \
              -d src/pipeline_steps/stage03_data_transformation.py \
              -o data/processed/X_test.csv \
              -o data/processed/X_train.csv \
              -o data/processed/y_test.csv \
              -o data/processed/y_train.csv \
              python src/pipeline_steps/stage03_data_transformation.py
```

```bash
dvc stage add -n model_trainer \
              -d src/config.yaml \
              -d data/processed/X_train.csv \
              -d data/processed/y_train.csv \
              -d src/pipeline_steps/stage04_model_trainer.py \
              python src/pipeline_steps/stage04_model_trainer.py
```

```bash
dvc stage add -n model_evaluation \
              -d src/config.yaml \
              -d data/processed/X_test.csv \
              -d data/processed/y_test.csv \
              -d models/model.joblib \
              -d src/pipeline_steps/stage05_model_evaluation.py \
              -o metrics/metrics.json \
              python src/pipeline_steps/stage05_model_evaluation.py
```

You can run the pipeline through the command `dvc repro`.

Congratulations! ðŸŽ‰ Now that you have a structured and well-defined MLOps project you're ready for the next step which is the creation of the API.

Each step is modularized, making it easy to maintain, extend, and scale your Machine Learning pipeline. 
